{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee20d1d5-05da-4704-b86b-7143bde2e24e",
   "metadata": {},
   "source": [
    "# Identifying Deforestation Drivers\n",
    "## INF367A Project\n",
    "\n",
    "<p style=\"text-align:right\"><b>Created by:</b> Halfdan Hesthammer, Lasse Holt and Tobias Huseb√∏</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Imports and initialization:\n",
    "\n",
    "#### Imports used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923af64b-eae0-4f4f-afd1-00ea5d7530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Machine Learning Libraries: #\n",
    "###############################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchmetrics import F1Score \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_convert\n",
    "from torchvision.transforms.functional import convert_image_dtype, to_pil_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "#################\n",
    "# Math imports: #\n",
    "#################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "\n",
    "########################################\n",
    "# Plotting / Image processing imports: #\n",
    "########################################\n",
    "import albumentations as albu\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf2f69-c7d5-47b2-884b-526c1a50018c",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "#### Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cde3b1-33e4-4169-8540-a1f861d58871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing seed, datatype and main computing unit:\n",
    "torch.manual_seed(69)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "data_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319ed6c-5bb1-4dcc-832b-e9870227f1c7",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Data Exploration:\n",
    "\n",
    "#### Printing Image Statistics Utilizing `rasterio` Package:\n",
    "\n",
    "##### Defining a .tif image opening function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de956d57-faa2-40e5-b3f2-f0feb066b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_tif_image(file_path):\n",
    "    \"\"\"Open a specific tif image and return file and array\"\"\"\n",
    "    src_file  = rasterio.open(file_path)\n",
    "    src_array = src_file.read()\n",
    "    return src_file, src_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7b968-4305-4f75-9dfd-46ccacd2453f",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "##### Opening and printing statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c05ed7-4a8e-4230-8152-b998bc5ff655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tif_image(file_path):\n",
    "    src_file, src_array = open_tif_image(file_path)\n",
    "\n",
    "    print(f\"Image shape (bands, height, width):\\n - {src_array.shape}\\n\")\n",
    "\n",
    "    print(f\"Image bands (index, dtype, nodataval, NaN count):\")\n",
    "    for i, dtype, band_array in zip(\n",
    "        src_file.indexes, src_file.dtypes, src_array\n",
    "    ):\n",
    "        nan_count = np.isnan(band_array).sum()\n",
    "        print(f\"Band {i}: dtype = {dtype}, NaN count = {nan_count}\")\n",
    "\n",
    "file_path = data_path + \"/train_images/train_0.tif\"\n",
    "\n",
    "analyze_tif_image(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df33f3-7c74-4119-9bbf-b78c7cdd9bc0",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "#### Opening Images utilizing `rasterio` package:\n",
    "\n",
    "##### Plotting every individual band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa442e-71fc-4dad-a78d-6d20c5738aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a 3x4 grid\n",
    "def visualize_bands(path):\n",
    "    _, src_array = open_tif_image(path)\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(15, 10))  # Adjust figsize for spacing\n",
    "    fig.suptitle(\"Visualizing Individual Bands\", fontsize=16, y=0.92)  # Main header\n",
    "\n",
    "    # Loop through bands and plot each one\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(src_array[i])      \n",
    "        ax.set_title(f\"Band {i + 1}\") \n",
    "        ax.axis('off')                 \n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)  \n",
    "    plt.show()\n",
    "\n",
    "visualize_bands(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044316af-684d-45ab-a2c5-ca9a4536d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_mean_std(image_dir):\n",
    "\n",
    "    band_sums = None\n",
    "    band_squared_sums = None\n",
    "    num_pixels = None  \n",
    "\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.tif'):\n",
    "            file_path = os.path.join(image_dir, filename)\n",
    "            with rasterio.open(file_path) as src:\n",
    "                image_array = src.read() \n",
    "            \n",
    "            mask = np.isnan(image_array) # Find nan_values to replace\n",
    "            image_array[mask] = 0  \n",
    "            \n",
    "            if band_sums is None:\n",
    "                band_sums = np.zeros(image_array.shape[0], dtype=np.float32)\n",
    "                band_squared_sums = np.zeros(image_array.shape[0], dtype=np.float32)\n",
    "                num_pixels = np.zeros(image_array.shape[0], dtype=np.float32)\n",
    "\n",
    "            band_sums += np.sum(image_array, axis=(1, 2))\n",
    "            band_squared_sums += np.sum(image_array**2, axis=(1, 2))\n",
    "            num_pixels += image_array.shape[1] * image_array.shape[2] \n",
    "\n",
    "    overall_means = band_sums / num_pixels\n",
    "    overall_stds = np.sqrt(band_squared_sums / num_pixels - overall_means**2)\n",
    "\n",
    "    return overall_means, overall_stds\n",
    "\n",
    "\n",
    "image_directory = data_path + \"/train_images\"\n",
    "overall_means, overall_stds = calculate_overall_mean_std(image_directory)\n",
    "\n",
    "for i, (mean, std) in enumerate(zip(overall_means, overall_stds), start=1):\n",
    "    print(f\"Band {i}: Overall Mean = {mean:.4f}, Overall Std = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ad6fc-f008-4eaa-b909-ce2366724a78",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "#### Looking at masks:\n",
    "\n",
    "##### Defining a .json opening and segementation retrieving function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c328c0c-4993-4ef4-9391-90da6562a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(file_path=data_path + \"/train_annotations.json\"):\n",
    "    with open(f\"{file_path}\") as f:\n",
    "        annotations = json.load(f)\n",
    "    return annotations\n",
    "\n",
    "def return_image_annotations(image_num, file_path=data_path + \"/train_annotations.json\"):\n",
    "    annotations = open_json(file_path)\n",
    "    return annotations[\"images\"][image_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3a1ea-800b-4771-8637-1eb6ed818f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training annotations for image 0:\n",
    "annotations_image_0 = return_image_annotations(0)\n",
    "\n",
    "header = \"Training Annotation Statistics\"\n",
    "print(f\"{header} \\n\" + f\"{'-' * len(header)}\")\n",
    "\n",
    "subtitle_1 = \"Keys and Values in Training Annotations:\"\n",
    "print(f\"\\n{(len(subtitle_1)+4)*'#'}\" + f\"\\n# {subtitle_1} #\\n\" + f\"{(len(subtitle_1)+4)*'#'}\\n\")\n",
    "for key, value in annotations_image_0.items():\n",
    "    value = str(value)\n",
    "    if len(value) > 25:\n",
    "        print(f\"- Key: {key}\")\n",
    "        print(f\"- Value: {value[:25]}... \\n\")\n",
    "    else:\n",
    "        print(f\"- Key: {key}\")\n",
    "        print(f\"- Value: {value:25s} \\n\")\n",
    "\n",
    "subtitle_2 = \"Segmentations with Class names:\"\n",
    "print(f\"\\n{(len(subtitle_2)+4)*'#'}\" + f\"\\n# {subtitle_2} #\\n\" + f\"{(len(subtitle_2)+4)*'#'}\\n\")\n",
    "for annotation in annotations_image_0.get(\"annotations\", []):\n",
    "    class_name = annotation[\"class\"]\n",
    "    segmentation = annotation[\"segmentation\"]\n",
    "    print(f\"- Class: {class_name}\")\n",
    "    print(f\"- Segmentation: {segmentation} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc5169-3bd2-4130-9185-2d2cfbdc8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes():\n",
    "    count = {}\n",
    "    area = {}\n",
    "    total_area_all = 0\n",
    "\n",
    "    for i in range(176):\n",
    "        image_anno = return_image_annotations(i)\n",
    "        total_area_image = 1024 * 1024  # Full image area\n",
    "        total_area_foreground = 0\n",
    "\n",
    "        for annotation in image_anno.get(\"annotations\", []):\n",
    "            class_name = annotation[\"class\"]\n",
    "            polygon = np.array(annotation[\"segmentation\"], dtype=np.int32).reshape(-1, 2)\n",
    "            poly_area = cv2.contourArea(polygon)\n",
    "            total_area_foreground += poly_area\n",
    "\n",
    "            if class_name in count:\n",
    "                count[class_name] += 1\n",
    "                area[class_name] += poly_area\n",
    "            else:\n",
    "                count[class_name] = 1\n",
    "                area[class_name] = poly_area\n",
    "\n",
    "        background_area = total_area_image - total_area_foreground\n",
    "        if \"background\" in area:\n",
    "            area[\"background\"] += background_area\n",
    "        else:\n",
    "            area[\"background\"] = background_area\n",
    "\n",
    "        total_area_all += total_area_image\n",
    "\n",
    "    return count, area\n",
    "\n",
    "counts, area = count_classes()\n",
    "\n",
    "labels = area.keys()\n",
    "values = area.values()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Total Area (pixels)\")\n",
    "plt.title(\"Pixel Area per Class (including background)\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "labels = counts.keys()\n",
    "values = counts.values()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Polygons\")\n",
    "plt.title(\"Count of Annotations per Class\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c0953-4bb4-4ff1-a5ce-8873bbbd277b",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "##### Tips for later:\n",
    "- Interpolate values for clouds with nearby pixels.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Create masks for labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f398997-f2fd-4959-866e-6191d68ffd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segmentation_masks(\n",
    "                            image_names: list[str], \n",
    "                              class_names: list[str], \n",
    "                              output_path, \n",
    "                              mask_shape=[4, 1024, 1024]):\n",
    "    \n",
    "    # Loads annotations\n",
    "    annotations = open_json()\n",
    "    \n",
    "    # Dict with filename as key and list of corresponding annotations as values\n",
    "    image_ann_dict = {img[\"file_name\"]: img[\"annotations\"] for img in annotations[\"images\"]}\n",
    "    \n",
    "    \n",
    "    for file_name in tqdm(image_names, desc=\"Generating masks\"):\n",
    "        # Initializes empty mask for all classes in each image\n",
    "        mask = np.zeros(mask_shape, dtype=np.uint8)\n",
    "\n",
    "        curr_annotations = image_ann_dict[file_name]\n",
    "\n",
    "        # Fill polygons for each class\n",
    "        for class_idx, class_name in enumerate(class_names): #Skips background class\n",
    "            class_annotations = [ann for ann in curr_annotations if ann[\"class\"] == class_name]\n",
    "            polygons = [ann[\"segmentation\"] for ann in class_annotations]\n",
    "        \n",
    "        # Draw each polygon on the corresponding class channel\n",
    "            for poly in polygons:\n",
    "                points = np.array(poly).astype(np.int32).reshape(-1, 2)\n",
    "                cv2.fillPoly(mask[class_idx], [points], 1)\n",
    "\n",
    "        driver_masks_combined = np.clip(mask[:4].sum(axis=0), 0, 1)\n",
    "        \n",
    "\n",
    "        # Save the mask as a .npy file\n",
    "        np.save(output_path + file_name.replace(\".tif\", \".npy\"), mask)\n",
    "\n",
    "    print(f\"Masks saved in {output_path}\")\n",
    "\n",
    "\n",
    "image_names = [f\"train_{i}.tif\" for i in range(176)]\n",
    "class_names = [\"plantation\", \"mining\", \"logging\", \"grassland_shrubland\"]\n",
    "\n",
    "create_segmentation_masks(image_names, class_names, data_path + \"/masks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05a118-19c6-4fb2-96be-bacf3fbfeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_npy_channels(file_path, class_names=class_names):\n",
    "    data = np.load(file_path)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))  # 2x2 grid\n",
    "    fig.suptitle(\"Channels Visualization\", fontsize=16)\n",
    "\n",
    "    for i in range(4):\n",
    "        ax = axes[i // 2, i % 2]  # Determine position in 2x2 grid\n",
    "        ax.set_title(f\"{class_names[i]}\")\n",
    "        im = ax.imshow(data[i], cmap='viridis')  # Plot each channel\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)  # Add colorbar to each subplot\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"Height\")\n",
    "\n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Leave space for the suptitle\n",
    "    plt.show()\n",
    "# Example usage\n",
    "visualize_npy_channels(\"./data/masks/train_107.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90305743-e4c0-4017-8196-4f0f7c7e2425",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b70be-2f9e-4117-9592-8ac053899c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(mask_path):\n",
    "    mask = np.load(mask_path)  # (5, H, W), uint8\n",
    "    assert mask.shape == (4, 1024, 1024)\n",
    "    mask = mask.transpose(1, 2, 0)  # (H, W, 4) for augmentation\n",
    "    return mask.astype(np.float32)  # normalize to [0, 1]\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        image = src.read() # Shape: (C, H, W)\n",
    "    \n",
    "    assert image.shape == (12, 1024, 1024)\n",
    "    image = np.nan_to_num(image, nan=0)\n",
    "\n",
    "    image = image.transpose(1, 2, 0) # (H, W, 4) for augmentation\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def normalize_image(image, mean=overall_means, std = overall_stds):\n",
    "    mean = mean.reshape(12, 1, 1)\n",
    "    std = std.reshape(12, 1, 1)\n",
    "    return (image - mean) / std\n",
    "\n",
    "\n",
    "class TrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, sample_indices, augmentations=None):\n",
    "        self.image_paths = [Path(data_root) / \"train_images\" / f\"train_{i}.tif\" for i in sample_indices]\n",
    "        self.mask_paths = [Path(data_root) / \"masks\" / f\"train_{i}.npy\" for i in sample_indices]\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = load_image(self.image_paths[idx])  # Load actual image\n",
    "        mask = load_mask(self.mask_paths[idx])  # Load actual mask\n",
    "\n",
    "        # Apply augmentations if provided\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image, mask=mask)\n",
    "            image, mask = augmented['image'], augmented['mask']\n",
    "\n",
    "        # Convert (H, W, C) -> (C, H, W)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        mask = mask.transpose(2, 0, 1)\n",
    "\n",
    "        # Normalize\n",
    "        image = normalize_image(image)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        return image, mask  # No more string paths\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075a4c6",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c7a6e-5b0e-4742-899d-7a5020e38d54",
   "metadata": {},
   "source": [
    "## Pixel based F1Score \n",
    "\n",
    "Should probably be a hybrid between f1 and dice/cross entropy,\n",
    "Focal lose if the dataset is to imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640be984-193e-44a7-bef7-e1e0fdabf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dice_loss = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "        self.bce_loss = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.0)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        dice = self.dice_loss(y_pred, y_true)\n",
    "        bce = self.bce_loss(y_pred, y_true)\n",
    "        return 0.7 * dice + 0.3 * bce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a1b43-b28f-483d-942c-4f3a028cb7b7",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c3361-0f99-44f3-8cbc-b53dfa2fd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=4):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"unet\", \n",
    "            in_channels=12,\n",
    "            classes=4,\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"UNet\"\n",
    "        \n",
    "class FPNNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=4):\n",
    "        super(FPNNet, self).__init__()\n",
    "\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"fpn\", \n",
    "            in_channels=in_channels,\n",
    "            classes=out_channels,\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"FPNet\"\n",
    "\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=4):\n",
    "        super(DeepNet, self).__init__()\n",
    "\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"deeplabv3\", \n",
    "            in_channels=in_channels,\n",
    "            classes=out_channels,\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"DeepNet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8625030-a2a2-40e3-8c8c-f70df502af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(train_losses, val_losses, title=\"Loss Curve\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def clear_cache():\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001, loss_fn=DiceBCELoss()):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            masks = masks.float()\n",
    "            images = torch.nan_to_num(images, nan=0.0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()/len(val_loader)\n",
    "            del images, masks, outputs\n",
    "            clear_cache()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "\n",
    "        # üß™ Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\"):\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                images = torch.nan_to_num(images, nan=0.0)\n",
    "                outputs = model(images)\n",
    "                val_loss += loss_fn(outputs, masks).item()\n",
    "\n",
    "                del images, masks, outputs\n",
    "                clear_cache()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        validation_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val:\n",
    "            best_weights = model.state_dict()\n",
    "            best_val = avg_val_loss\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    plot_loss_curve(training_losses, validation_losses)\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4aa3d-d2ac-4335-9848-537ec46fb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "train_indices = list(range(0, 140))\n",
    "val_indices = list(range(140, 176))\n",
    "train_dataset = TrainingDataset(data_path, train_indices, augmentations=None)\n",
    "val_dataset = TrainingDataset(data_path, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "model1 = UNet()\n",
    "model2 = FPNNet()\n",
    "model3 = DeepNet()\n",
    "models = [model1, model2, model3]\n",
    "model_loss = [0, 0, 0]\n",
    "train = True\n",
    "for i in range(0, len(models)):\n",
    "    model = models[i]\n",
    "    if train:\n",
    "        print(f\"Training model {model}\")\n",
    "        best_loss = train_model(model, train_loader, val_loader, num_epochs=3)\n",
    "        model_loss[i] = best_loss\n",
    "        torch.save(model.state_dict(), f\"{model}_weights.pth\")\n",
    "    else:\n",
    "        print(\"Load pretrained weights\")\n",
    "        model.load_state_dict(torch.load(f\"{model}_weights.pth\"))\n",
    "\n",
    "best_index = model_loss.index(min(model_loss))\n",
    "best_model = models[best_index]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70d3dc-76f6-4746-be4e-298dd77434a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_predictions(model, data_loader, device, num_images):\n",
    "    model.eval()  # Set model to eval mode\n",
    "    indices = random.sample(range(len(data_loader.dataset)), num_images)  # Pick random indices\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            # Get one image & mask from dataset\n",
    "            image, mask = data_loader.dataset[idx]  # Assuming dataset implements __getitem__()\n",
    "            \n",
    "\n",
    "            image = image.to(device).unsqueeze(0) \n",
    "            mask = mask.to(device)  # Shape: (4, H, W)\n",
    "            \n",
    "\n",
    "            # Get model prediction\n",
    "            #predicted_mask = model(image)  # Shape: (1, 4, 1024, 1024)\n",
    "            #predicted_mask = predicted_mask.squeeze(0).cpu().numpy()  # Remove batch dim -> (4, 1024, 1024)\n",
    "            # Get model prediction\n",
    "            logits = model(image)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            predicted_mask = (probs > 0.5).float().squeeze(0).cpu().numpy()\n",
    "            \n",
    "\n",
    "            mask = mask.cpu().numpy()\n",
    "            fig, axes = plt.subplots(2, 4, figsize=(16, 8))  # 2 rows, 4 columns\n",
    "            class_names = [\"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\"]  # Adjust as needed\n",
    "\n",
    "            for class_idx in range(4):  # Iterate over each class\n",
    "                # True Mask (Top Row)\n",
    "                axes[0, class_idx].imshow(mask[class_idx], cmap='gray')  \n",
    "                axes[0, class_idx].set_title(f'True Mask (Class {class_idx})')\n",
    "                axes[0, class_idx].axis('off')\n",
    "                \n",
    "\n",
    "                # Predicted Mask (Bottom Row)\n",
    "                axes[1, class_idx].imshow(predicted_mask[class_idx], cmap='gray', vmin=0, vmax=1)\n",
    "  \n",
    "                axes[1, class_idx].set_title(f'Predicted Mask (Class {class_idx})')\n",
    "                axes[1, class_idx].axis('off')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "visualize_random_predictions(models[0], val_loader, device, num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe26a6",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, sample_indices):\n",
    "        self.image_paths = [Path(data_root) / \"evaluation_images\" / f\"evaluation_{i}.tif\" \n",
    "                            for i in sample_indices]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = load_image(image_path)  # Load actual image\n",
    "        \n",
    "        # Convert (H, W, C) -> (C, H, W)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        # Normalize\n",
    "        image = normalize_image(image)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # file name as string for the submission file\n",
    "        file_name = image_path.name\n",
    "        \n",
    "        return {\"image\": image, \"image_path\": file_name}\n",
    "    \n",
    "# for handling non-tensor objects when stacking samples into batches\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"image\": torch.stack([b[\"image\"] for b in batch]),\n",
    "        \"image_path\": [b[\"image_path\"] for b in batch]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1e3a2",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4687112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, loader, output_dir):\n",
    "    # convert to path object\n",
    "    output_dir = Path(output_dir)\n",
    "    # make dir if needed\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    # set model to eval \n",
    "    model.eval().to(device)\n",
    "    \n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Predicting\"):\n",
    "        image = batch[\"image\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get prediction\n",
    "            logits = model(image)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        for name, p in zip(batch[\"image_path\"], probs):\n",
    "            np.save(output_dir / name.replace(\".tif\", \".npy\"),\n",
    "                    p.astype(np.float16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a408de-32ce-4086-9448-04c16a3860e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------\n",
    "# Polygon Conversion\n",
    "# -------------------\n",
    "\n",
    "def mask_to_polygons(mask):\n",
    "    \"\"\"Convert a binary mask to polygon segmentations\"\"\"\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        if contour.shape[0] >= 3:\n",
    "            polygon = contour.squeeze().flatten()\n",
    "            if len(polygon) >= 6:\n",
    "                polygons.append([int(p) for p in polygon])\n",
    "    return polygons\n",
    "\n",
    "def generate_annotations_for_image(file_name, mask, class_names):\n",
    "    \"\"\"Generate annotations per class from predicted mask\"\"\"\n",
    "    annotations = []\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_mask = mask[class_idx]\n",
    "        for polygon in mask_to_polygons(class_mask):\n",
    "            annotations.append({\n",
    "                \"class\": class_name,\n",
    "                \"segmentation\": polygon\n",
    "            })\n",
    "    return {\"file_name\": file_name, \n",
    "            \"annotations\": annotations} \n",
    "\n",
    "def npy_to_submission(pred_dir, out_json,\n",
    "                    score_thresh=0.5, min_area=10):\n",
    "    class_names = [\"plantation\",\"mining\",\n",
    "                   \"logging\",\"grassland_shrubland\"]\n",
    "    images = []\n",
    "\n",
    "    for npy_path in sorted(Path(pred_dir).glob(\"*.npy\")):\n",
    "        probs = np.load(npy_path)                # (4,H,W)\n",
    "        binary = (probs > score_thresh).astype(np.uint8)\n",
    "\n",
    "        # optional tiny-area filter\n",
    "        for ci in range(4):\n",
    "            if binary[ci].sum() < min_area:\n",
    "                binary[ci][:] = 0\n",
    "\n",
    "        images.append(\n",
    "            generate_annotations_for_image(\n",
    "                file_name = npy_path.stem + \".tif\",\n",
    "                mask      = binary,\n",
    "                class_names=class_names)\n",
    "        )\n",
    "\n",
    "    with open(out_json,\"w\",encoding=\"utf-8\") as fp:\n",
    "        json.dump({\"images\": images}, fp, indent=4)\n",
    "    print(f\"Submission saved -> {out_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet submission\n",
    "\n",
    "model = best_model\n",
    "test_indices = list(range(0, 118))\n",
    "pred_dir = data_path + \"predictions_unet\"\n",
    "submission_out = data_path + \"submission.json\"\n",
    "test_dataset = TestDataset(data_path, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "make_predictions(model, test_loader, pred_dir) \n",
    "npy_to_submission(pred_dir, submission_out, 0.5, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------\n",
    "# Prediction Loop\n",
    "# -------------------\n",
    "\n",
    "def generate_annotations(model, image_dir, output_json_path, device):\n",
    "    class_names = [\"plantation\", \"mining\", \"logging\", \"grassland_shrubland\"]\n",
    "    all_annotations = []\n",
    "\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    for i, file_name in enumerate(tqdm(image_files, desc=\"Generating annotations\")):\n",
    "        image_path = image_dir + file_name\n",
    "\n",
    "        # Load image\n",
    "        image = load_image(image_path)  # shape: (H, W, C)\n",
    "        image_tensor = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(image_tensor)\n",
    "            if logits.dim() == 4 and logits.shape[0] == 1:\n",
    "                logits = logits.squeeze(0)  # shape: [4, H, W]\n",
    "            probs = torch.sigmoid(logits)\n",
    "            predicted_mask = (probs > 0.5).float().cpu().numpy()  # shape: [4, H, W]\n",
    "\n",
    "        # Generate per-image annotation\n",
    "        result = generate_annotations_for_image(file_name, predicted_mask, class_names)\n",
    "        all_annotations.append(result)\n",
    "\n",
    "    # Write to JSON safely\n",
    "    json_str = json.dumps({\"images\": all_annotations}, indent=4)\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "    print(f\"Saved to: {output_json_path}\")\n",
    "\n",
    "generate_annotations(model, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b15f3-2544-41f2-aee7-4cd37fc04202",
   "metadata": {},
   "outputs": [],
   "source": [
    "for an in all_annotations:\n",
    "    print(an)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76a10a-88bd-4167-9b07-95d18a3d9196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
